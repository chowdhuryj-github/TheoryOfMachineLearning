{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42858b02",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'JetBrains Mono', monospace; font-size:16px;\">\n",
    "\n",
    "# Problem Set: Cost Function Fun\n",
    "In this problem, we understand how regularization (Ridge & Lasso) helps to prevent overfitting by shrinking model weights, and how changing the regularization strength affects training error, test error and the bias-variance trade-off. Using a synthetic daataset, we fit the Ridge & Lasso models with different regularization values, look at the effect on coefficient norms and use cross-validation to pick the best model and evaluate it. \n",
    "\n",
    "## Regularization Hyperparameter Questions\n",
    "1. What is the difference between a model's parameters and a model's hyperparameters?\n",
    "- **Parameter:** this is something the model learns from the data. For example, the weight coefficients in linear regression\n",
    "- **Hyperparameter:** this is something we choose before training. For example, the alpha (learning rate) or the number of trees in a random forest.\n",
    "\n",
    "2. As we incease (λ) from 0, how will the training MSE (mean-squared error) change? How will the test MSE change? Sketch the bias-variance trade-off?\n",
    "As we increase the λ from 0, the training MSE increases because stronger regularization forces the model to fit the training data less perfectly. The testing MSE first decreases (less overfitting) and then increases (underfitting), forming a U-shaped curve.   \n",
    "\n",
    "3. If training and validation errors are both high and almost equal, should you increase or decrease λ?\n",
    "We should decrease λ because high training and high validation errors means the model is underfitting. Underfitting happens when the λ is too large and lowering λ gives the model more freedom to learn patterns. \n",
    "\n",
    "## The Different Kinds of Regression\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "\n",
    "    - generate a regression dataset\n",
    "    - split data into 80% for training and 20% for testing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# other relevant imports\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=50, n_informative=10, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "# range of alpha values\n",
    "alphas = np.arange(0, 100, 0.1)\n",
    "coefficient_norms = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    model = Pipeline([\n",
    "        \n",
    "        # scale the data and apply ridge\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    # fitting the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # extracting coefficients\n",
    "    coefficients = model.named_steps[\"ridge\"].coef_\n",
    "\n",
    "    # find L2 norm\n",
    "    norm = np.linalg.norm(coefficients)\n",
    "    coefficient_norms.append(norm)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs3400_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
