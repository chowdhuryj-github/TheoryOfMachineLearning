{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42858b02",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'JetBrains Mono', monospace; font-size:16px;\">\n",
    "\n",
    "# Problem Set: Cost Function Fun\n",
    "In this problem, we understand how regularization (Ridge & Lasso) helps to prevent overfitting by shrinking model weights, and how changing the regularization strength affects training error, test error and the bias-variance trade-off. Using a synthetic daataset, we fit the Ridge & Lasso models with different regularization values, look at the effect on coefficient norms and use cross-validation to pick the best model and evaluate it. \n",
    "\n",
    "## Problem 1: Regularization\n",
    "\n",
    "### Regularization Hyperparameter Questions\n",
    "1. What is the difference between a model's parameters and a model's hyperparameters?\n",
    "- **Parameter:** this is something the model learns from the data. For example, the weight coefficients in linear regression\n",
    "- **Hyperparameter:** this is something we choose before training. For example, the alpha (learning rate) or the number of trees in a random forest.\n",
    "\n",
    "2. As we incease (λ) from 0, how will the training MSE (mean-squared error) change? How will the test MSE change? Sketch the bias-variance trade-off?\n",
    "\n",
    "As we increase the λ from 0, the training MSE increases because stronger regularization forces the model to fit the training data less perfectly. The testing MSE first decreases (less overfitting) and then increases (underfitting), forming a U-shaped curve.   \n",
    "\n",
    "3. If training and validation errors are both high and almost equal, should you increase or decrease λ?\n",
    "\n",
    "We should decrease λ because high training and high validation errors means the model is underfitting. Underfitting happens when the λ is too large and lowering λ gives the model more freedom to learn patterns. \n",
    "\n",
    "## The Different Kinds of Regression\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c695eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient Norms:  [np.float64(130.75113707973009), np.float64(130.63842221489824), np.float64(130.52602236479603), np.float64(130.41393567377773), np.float64(130.30216030371352), np.float64(130.1906944337651), np.float64(130.07953626016456), np.float64(129.96868399599708), np.float64(129.85813587098684), np.float64(129.74789013128657), np.float64(129.6379450392699), np.float64(129.5282988733273), np.float64(129.41894992766564), np.float64(129.30989651210967), np.float64(129.2011369519081), np.float64(129.09266958754128), np.float64(128.9844927745328), np.float64(128.87660488326364), np.float64(128.76900429878907), np.float64(128.66168942065872), np.float64(128.5546586627388), np.float64(128.44791045303774), np.float64(128.34144323353382), np.float64(128.23525546000613), np.float64(128.12934560186713), np.float64(128.0237121419987), np.float64(127.91835357659008), np.float64(127.81326841497814), np.float64(127.7084551794908), np.float64(127.60391240529164), np.float64(127.49963864022781), np.float64(127.39563244467988), np.float64(127.29189239141341), np.float64(127.18841706543357), np.float64(127.08520506384126), np.float64(126.98225499569146), np.float64(126.87956548185392), np.float64(126.77713515487564), np.float64(126.67496265884516), np.float64(126.5730466492594), np.float64(126.47138579289182), np.float64(126.3699787676627), np.float64(126.26882426251161), np.float64(126.16792097727118), np.float64(126.06726762254286), np.float64(125.96686291957467), np.float64(125.8667056001404), np.float64(125.76679440642071), np.float64(125.66712809088578), np.float64(125.56770541617968), np.float64(125.46852515500649), np.float64(125.36958609001763), np.float64(125.2708870137013), np.float64(125.17242672827302), np.float64(125.07420404556797), np.float64(124.97621778693461), np.float64(124.87846678312985), np.float64(124.78094987421596), np.float64(124.68366590945834), np.float64(124.58661374722516), np.float64(124.48979225488807), np.float64(124.39320030872459), np.float64(124.2968367938215), np.float64(124.20070060397971), np.float64(124.10479064162045), np.float64(124.00910581769259), np.float64(123.91364505158141), np.float64(123.81840727101849), np.float64(123.72339141199242), np.float64(123.62859641866181), np.float64(123.53402124326811), np.float64(123.43966484605045), np.float64(123.34552619516138), np.float64(123.25160426658381), np.float64(123.15789804404874), np.float64(123.06440651895451), np.float64(122.9711286902868), np.float64(122.87806356453981), np.float64(122.7852101556385), np.float64(122.69256748486158), np.float64(122.60013458076597), np.float64(122.50791047911186), np.float64(122.41589422278868), np.float64(122.32408486174263), np.float64(122.23248145290437), np.float64(122.14108306011796), np.float64(122.04988875407115), np.float64(121.95889761222556), np.float64(121.86810871874884), np.float64(121.77752116444704), np.float64(121.68713404669775), np.float64(121.59694646938475), np.float64(121.50695754283277), np.float64(121.41716638374346), np.float64(121.32757211513203), np.float64(121.23817386626479), np.float64(121.1489707725974), np.float64(121.0599619757138), np.float64(120.97114662326614), np.float64(120.8825238689152), np.float64(120.7940928722718), np.float64(120.70585279883854), np.float64(120.61780281995273), np.float64(120.52994211272983), np.float64(120.44226986000749), np.float64(120.35478525029018), np.float64(120.26748747769486), np.float64(120.1803757418971), np.float64(120.09344924807786), np.float64(120.0067072068708), np.float64(119.92014883431048), np.float64(119.833773351781), np.float64(119.74757998596515), np.float64(119.66156796879473), np.float64(119.57573653740076), np.float64(119.49008493406448), np.float64(119.40461240616945), np.float64(119.31931820615334), np.float64(119.23420159146119), np.float64(119.14926182449857), np.float64(119.06449817258542), np.float64(118.9799099079108), np.float64(118.89549630748766), np.float64(118.8112566531087), np.float64(118.72719023130199), np.float64(118.64329633328792), np.float64(118.55957425493607), np.float64(118.47602329672286), np.float64(118.39264276368974), np.float64(118.30943196540156), np.float64(118.22639021590557), np.float64(118.14351683369117), np.float64(118.06081114164965), np.float64(117.97827246703477), np.float64(117.89590014142351), np.float64(117.81369350067742), np.float64(117.73165188490455), np.float64(117.64977463842135), np.float64(117.56806110971567), np.float64(117.48651065140943), np.float64(117.40512262022227), np.float64(117.32389637693548), np.float64(117.2428312863562), np.float64(117.16192671728213), np.float64(117.08118204246654), np.float64(117.00059663858372), np.float64(116.92016988619514), np.float64(116.83990116971506), np.float64(116.75978987737783), np.float64(116.67983540120429), np.float64(116.60003713696939), np.float64(116.52039448416994), np.float64(116.44090684599236), np.float64(116.3615736292814), np.float64(116.28239424450864), np.float64(116.20336810574166), np.float64(116.12449463061357), np.float64(116.04577324029249), np.float64(115.96720335945186), np.float64(115.88878441624074), np.float64(115.81051584225467), np.float64(115.73239707250633), np.float64(115.6544275453973), np.float64(115.57660670268945), np.float64(115.4989339894771), np.float64(115.42140885415895), np.float64(115.344030748411), np.float64(115.26679912715913), np.float64(115.18971344855224), np.float64(115.1127731739358), np.float64(115.03597776782543), np.float64(114.95932669788085), np.float64(114.88281943488026), np.float64(114.80645545269451), np.float64(114.73023422826236), np.float64(114.65415524156505), np.float64(114.578217975602), np.float64(114.5024219163659), np.float64(114.42676655281898), np.float64(114.35125137686883), np.float64(114.27587588334461), np.float64(114.20063956997384), np.float64(114.12554193735902), np.float64(114.05058248895466), np.float64(113.97576073104463), np.float64(113.90107617271953), np.float64(113.82652832585448), np.float64(113.75211670508703), np.float64(113.67784082779546), np.float64(113.60370021407697), np.float64(113.52969438672636), np.float64(113.45582287121488), np.float64(113.38208519566942), np.float64(113.30848089085129), np.float64(113.23500949013626), np.float64(113.16167052949375), np.float64(113.08846354746693), np.float64(113.01538808515281), np.float64(112.94244368618232), np.float64(112.86962989670089), np.float64(112.796946265349), np.float64(112.72439234324322), np.float64(112.65196768395701), np.float64(112.57967184350207), np.float64(112.50750438030961), np.float64(112.43546485521216), np.float64(112.36355283142508), np.float64(112.29176787452857), np.float64(112.22010955244997), np.float64(112.1485774354457), np.float64(112.07717109608394), np.float64(112.00589010922734), np.float64(111.93473405201559), np.float64(111.8637025038485), np.float64(111.79279504636914), np.float64(111.72201126344704), np.float64(111.65135074116165), np.float64(111.58081306778612), np.float64(111.51039783377081), np.float64(111.44010463172731), np.float64(111.36993305641248), np.float64(111.29988270471274), np.float64(111.2299531756284), np.float64(111.16014407025811), np.float64(111.09045499178355), np.float64(111.02088554545423), np.float64(110.95143533857255), np.float64(110.8821039804787), np.float64(110.81289108253593), np.float64(110.7437962581161), np.float64(110.67481912258482), np.float64(110.60595929328741), np.float64(110.5372163895346), np.float64(110.46859003258827), np.float64(110.40007984564755), np.float64(110.33168545383509), np.float64(110.26340648418322), np.float64(110.19524256562042), np.float64(110.12719332895774), np.float64(110.05925840687566), np.float64(109.9914374339106), np.float64(109.92373004644209), np.float64(109.85613588267957), np.float64(109.78865458264977), np.float64(109.72128578818345), np.float64(109.65402914290358), np.float64(109.58688429221202), np.float64(109.51985088327754), np.float64(109.45292856502346), np.float64(109.38611698811539), np.float64(109.31941580494924), np.float64(109.25282466963915), np.float64(109.18634323800559), np.float64(109.11997116756393), np.float64(109.05370811751241), np.float64(108.98755374872057), np.float64(108.92150772371812), np.float64(108.85556970668347), np.float64(108.7897393634323), np.float64(108.7240163614066), np.float64(108.65840036966351), np.float64(108.59289105886444), np.float64(108.52748810126425), np.float64(108.46219117070028), np.float64(108.396999942582), np.float64(108.33191409387999), np.float64(108.26693330311592), np.float64(108.20205725035187), np.float64(108.13728561718007), np.float64(108.07261808671274), np.float64(108.0080543435719), np.float64(107.94359407387932), np.float64(107.87923696524659), np.float64(107.81498270676535), np.float64(107.75083098899724), np.float64(107.68678150396458), np.float64(107.62283394514026), np.float64(107.55898800743857), np.float64(107.49524338720569), np.float64(107.43159978221023), np.float64(107.36805689163386), np.float64(107.30461441606224), np.float64(107.24127205747584), np.float64(107.17802951924072), np.float64(107.1148865060999), np.float64(107.05184272416393), np.float64(106.98889788090246), np.float64(106.92605168513532), np.float64(106.86330384702389), np.float64(106.8006540780623), np.float64(106.73810209106901), np.float64(106.67564760017842), np.float64(106.61329032083233), np.float64(106.55102996977152), np.float64(106.48886626502767), np.float64(106.42679892591507), np.float64(106.36482767302233), np.float64(106.30295222820475), np.float64(106.24117231457565), np.float64(106.17948765649908), np.float64(106.11789797958146), np.float64(106.05640301066411), np.float64(105.99500247781528), np.float64(105.93369611032254), np.float64(105.87248363868518), np.float64(105.81136479460665), np.float64(105.75033931098687), np.float64(105.68940692191515), np.float64(105.62856736266255), np.float64(105.56782036967451), np.float64(105.50716568056377), np.float64(105.44660303410312), np.float64(105.3861321702181), np.float64(105.32575282998013), np.float64(105.26546475559931), np.float64(105.20526769041747), np.float64(105.14516137890129), np.float64(105.08514556663548), np.float64(105.02522000031574), np.float64(104.96538442774231), np.float64(104.90563859781297), np.float64(104.84598226051662), np.float64(104.78641516692643), np.float64(104.72693706919357), np.float64(104.66754772054043), np.float64(104.60824687525445), np.float64(104.5490342886814), np.float64(104.48990971721929), np.float64(104.43087291831202), np.float64(104.37192365044297), np.float64(104.31306167312896), np.float64(104.254286746914), np.float64(104.19559863336325), np.float64(104.13699709505694), np.float64(104.07848189558429), np.float64(104.02005279953755), np.float64(103.96170957250627), np.float64(103.90345198107102), np.float64(103.8452797927979), np.float64(103.78719277623277), np.float64(103.72919070089515), np.float64(103.67127333727282), np.float64(103.61344045681616), np.float64(103.55569183193238), np.float64(103.49802723597995), np.float64(103.44044644326324), np.float64(103.38294922902683), np.float64(103.32553536945024), np.float64(103.2682046416423), np.float64(103.21095682363605), np.float64(103.15379169438313), np.float64(103.09670903374871), np.float64(103.03970862250613), np.float64(102.98279024233172), np.float64(102.92595367579956), np.float64(102.86919870637652), np.float64(102.81252511841691), np.float64(102.75593269715765), np.float64(102.69942122871319), np.float64(102.64299050007031), np.float64(102.58664029908361), np.float64(102.53037041447007), np.float64(102.47418063580466), np.float64(102.41807075351514), np.float64(102.36204055887748), np.float64(102.30608984401091), np.float64(102.25021840187333), np.float64(102.19442602625664), np.float64(102.13871251178186), np.float64(102.08307765389459), np.float64(102.02752124886062), np.float64(101.97204309376103), np.float64(101.91664298648787), np.float64(101.86132072573959), np.float64(101.80607611101657), np.float64(101.7509089426167), np.float64(101.69581902163094), np.float64(101.64080614993898), np.float64(101.58587013020494), np.float64(101.5310107658729), np.float64(101.47622786116283), np.float64(101.42152122106603), np.float64(101.36689065134141), np.float64(101.31233595851072), np.float64(101.25785694985485), np.float64(101.20345343340935), np.float64(101.14912521796064), np.float64(101.09487211304163), np.float64(101.04069392892798), np.float64(100.98659047663372), np.float64(100.93256156790771), np.float64(100.87860701522926), np.float64(100.8247266318044), np.float64(100.77092023156197), np.float64(100.71718762914969), np.float64(100.66352863993039), np.float64(100.60994307997797), np.float64(100.55643076607383), np.float64(100.50299151570317), np.float64(100.44962514705085), np.float64(100.39633147899792), np.float64(100.34311033111813), np.float64(100.28996152367378), np.float64(100.2368848776125), np.float64(100.1838802145634), np.float64(100.13094735683354), np.float64(100.07808612740432), np.float64(100.02529634992798), np.float64(99.97257784872409), np.float64(99.919930448776), np.float64(99.86735397572737), np.float64(99.8148482558787), np.float64(99.76241311618396), np.float64(99.71004838424724), np.float64(99.65775388831915), np.float64(99.60552945729363), np.float64(99.55337492070466), np.float64(99.50129010872263), np.float64(99.44927485215158), np.float64(99.3973289824254), np.float64(99.3454523316049), np.float64(99.29364473237437), np.float64(99.24190601803862), np.float64(99.19023602251966), np.float64(99.13863458035345), np.float64(99.08710152668682), np.float64(99.03563669727453), np.float64(98.98423992847593), np.float64(98.93291105725181), np.float64(98.88164992116172), np.float64(98.83045635836059), np.float64(98.77933020759585), np.float64(98.72827130820427), np.float64(98.67727950010931), np.float64(98.6263546238178), np.float64(98.57549652041716), np.float64(98.52470503157248), np.float64(98.47397999952366), np.float64(98.42332126708237), np.float64(98.37272867762933), np.float64(98.3222020751114), np.float64(98.27174130403868), np.float64(98.22134620948196), np.float64(98.17101663706954), np.float64(98.12075243298479), np.float64(98.07055344396312), np.float64(98.02041951728951), np.float64(97.97035050079562), np.float64(97.92034624285705), np.float64(97.87040659239081), np.float64(97.8205313988525), np.float64(97.7707205122337), np.float64(97.7209737830594), np.float64(97.67129106238534), np.float64(97.62167220179532), np.float64(97.57211705339874), np.float64(97.52262546982789), np.float64(97.47319730423564), np.float64(97.42383241029256), np.float64(97.37453064218478), np.float64(97.32529185461104), np.float64(97.27611590278067), np.float64(97.22700264241068), np.float64(97.17795192972373), np.float64(97.12896362144531), np.float64(97.08003757480137), np.float64(97.0311736475163), np.float64(96.98237169780984), np.float64(96.93363158439548), np.float64(96.88495316647742), np.float64(96.83633630374855), np.float64(96.78778085638812), np.float64(96.73928668505926), np.float64(96.69085365090683), np.float64(96.64248161555497), np.float64(96.5941704411049), np.float64(96.54591999013266), np.float64(96.4977301256869), np.float64(96.44960071128652), np.float64(96.4015316109184), np.float64(96.3535226890355), np.float64(96.30557381055424), np.float64(96.25768484085265), np.float64(96.20985564576796), np.float64(96.16208609159465), np.float64(96.11437604508215), np.float64(96.06672537343273), np.float64(96.01913394429936), np.float64(95.97160162578376), np.float64(95.92412828643405), np.float64(95.87671379524295), np.float64(95.8293580216454), np.float64(95.78206083551677), np.float64(95.73482210717073), np.float64(95.68764170735706), np.float64(95.64051950725984), np.float64(95.59345537849536), np.float64(95.54644919311013), np.float64(95.49950082357893), np.float64(95.45261014280265), np.float64(95.40577702410663), np.float64(95.3590013412384), np.float64(95.31228296836605), np.float64(95.26562178007606), np.float64(95.21901765137144), np.float64(95.1724704576699), np.float64(95.12598007480193), np.float64(95.0795463790088), np.float64(95.0331692469409), np.float64(94.98684855565574), np.float64(94.94058418261608), np.float64(94.89437600568812), np.float64(94.84822390313978), np.float64(94.80212775363879), np.float64(94.75608743625072), np.float64(94.71010283043763), np.float64(94.66417381605568), np.float64(94.61830027335392), np.float64(94.57248208297224), np.float64(94.52671912593961), np.float64(94.48101128367236), np.float64(94.43535843797258), np.float64(94.38976047102616), np.float64(94.34421726540128), np.float64(94.29872870404664), np.float64(94.25329467028969), np.float64(94.20791504783503), np.float64(94.16258972076268), np.float64(94.11731857352656), np.float64(94.0721014909526), np.float64(94.02693835823723), np.float64(93.9818290609458), np.float64(93.93677348501073), np.float64(93.89177151673019), np.float64(93.84682304276627), np.float64(93.80192795014344), np.float64(93.75708612624695), np.float64(93.71229745882131), np.float64(93.66756183596861), np.float64(93.62287914614707), np.float64(93.57824927816942), np.float64(93.5336721212013), np.float64(93.48914756475986), np.float64(93.4446754987121), np.float64(93.40025581327339), np.float64(93.35588839900602), np.float64(93.31157314681762), np.float64(93.26730994795969), np.float64(93.22309869402602), np.float64(93.17893927695148), np.float64(93.13483158901023), np.float64(93.09077552281445), np.float64(93.04677097131281), np.float64(93.00281782778906), np.float64(92.95891598586051), np.float64(92.9150653394768), np.float64(92.8712657829182), np.float64(92.82751721079447), np.float64(92.78381951804316), np.float64(92.74017259992856), np.float64(92.69657635203993), np.float64(92.65303067029045), np.float64(92.60953545091559), np.float64(92.56609059047199), np.float64(92.52269598583575), np.float64(92.47935153420148), np.float64(92.43605713308057), np.float64(92.39281268030028), np.float64(92.34961807400192), np.float64(92.30647321263989), np.float64(92.26337799498025), np.float64(92.22033232009933), np.float64(92.17733608738257), np.float64(92.1343891965231), np.float64(92.09149154752053), np.float64(92.04864304067965), np.float64(92.00584357660908), np.float64(91.96309305622009), np.float64(91.92039138072542), np.float64(91.87773845163768), np.float64(91.83513417076861), np.float64(91.79257844022742), np.float64(91.75007116241964), np.float64(91.70761224004607), np.float64(91.66520157610144), np.float64(91.62283907387312), np.float64(91.58052463694003), np.float64(91.53825816917137), np.float64(91.4960395747255), np.float64(91.45386875804859), np.float64(91.41174562387366), np.float64(91.36967007721925), np.float64(91.32764202338826), np.float64(91.28566136796678), np.float64(91.24372801682308), np.float64(91.20184187610619), np.float64(91.16000285224506), np.float64(91.11821085194703), np.float64(91.0764657821972), np.float64(91.03476755025684), np.float64(90.99311606366254), np.float64(90.95151123022495), np.float64(90.90995295802774), np.float64(90.86844115542654), np.float64(90.82697573104772), np.float64(90.78555659378732), np.float64(90.74418365281015), np.float64(90.70285681754834), np.float64(90.66157599770071), np.float64(90.62034110323128), np.float64(90.57915204436851), np.float64(90.53800873160398), np.float64(90.49691107569166), np.float64(90.45585898764664), np.float64(90.41485237874397), np.float64(90.37389116051789), np.float64(90.33297524476072), np.float64(90.29210454352173), np.float64(90.25127896910611), np.float64(90.21049843407421), np.float64(90.16976285124016), np.float64(90.12907213367109), np.float64(90.08842619468619), np.float64(90.04782494785546), np.float64(90.00726830699892), np.float64(89.96675618618555), np.float64(89.92628849973237), np.float64(89.8858651622033), np.float64(89.84548608840846), np.float64(89.80515119340293), np.float64(89.76486039248597), np.float64(89.7246136011999), np.float64(89.68441073532934), np.float64(89.64425171090019), np.float64(89.6041364441785), np.float64(89.56406485166988), np.float64(89.52403685011828), np.float64(89.48405235650515), np.float64(89.44411128804862), np.float64(89.4042135622024), np.float64(89.36435909665497), np.float64(89.32454780932868), np.float64(89.28477961837882), np.float64(89.24505444219261), np.float64(89.20537219938856), np.float64(89.16573280881538), np.float64(89.12613618955106), np.float64(89.08658226090218), np.float64(89.04707094240277), np.float64(89.00760215381369), np.float64(88.9681758151217), np.float64(88.9287918465383), np.float64(88.88945016849945), np.float64(88.85015070166403), np.float64(88.81089336691363), np.float64(88.77167808535114), np.float64(88.73250477830034), np.float64(88.69337336730484), np.float64(88.65428377412722), np.float64(88.61523592074832), np.float64(88.57622972936636), np.float64(88.53726512239612), np.float64(88.49834202246797), np.float64(88.45946035242741), np.float64(88.42062003533383), np.float64(88.38182099446009), np.float64(88.34306315329134), np.float64(88.30434643552462), np.float64(88.26567076506767), np.float64(88.22703606603845), np.float64(88.18844226276417), np.float64(88.14988927978048), np.float64(88.11137704183093), np.float64(88.07290547386592), np.float64(88.03447450104197), np.float64(87.99608404872116), np.float64(87.95773404247012), np.float64(87.91942440805938), np.float64(87.88115507146253), np.float64(87.84292595885567), np.float64(87.80473699661631), np.float64(87.76658811132299), np.float64(87.72847922975429), np.float64(87.69041027888821), np.float64(87.65238118590126), np.float64(87.61439187816804), np.float64(87.57644228326018), np.float64(87.53853232894579), np.float64(87.50066194318866), np.float64(87.46283105414771), np.float64(87.42503959017598), np.float64(87.3872874798201), np.float64(87.34957465181971), np.float64(87.31190103510632), np.float64(87.27426655880315), np.float64(87.23667115222396), np.float64(87.19911474487269), np.float64(87.16159726644257), np.float64(87.12411864681548), np.float64(87.08667881606124), np.float64(87.0492777044371), np.float64(87.01191524238668), np.float64(86.9745913605397), np.float64(86.93730598971108), np.float64(86.90005906090035), np.float64(86.86285050529085), np.float64(86.82568025424928), np.float64(86.78854823932483), np.float64(86.75145439224863), np.float64(86.71439864493311), np.float64(86.67738092947131), np.float64(86.64040117813619), np.float64(86.60345932338001), np.float64(86.56655529783374), np.float64(86.52968903430637), np.float64(86.49286046578428), np.float64(86.45606952543055), np.float64(86.41931614658442), np.float64(86.38260026276062), np.float64(86.34592180764875), np.float64(86.30928071511262), np.float64(86.27267691918965), np.float64(86.23611035409029), np.float64(86.19958095419737), np.float64(86.1630886540654), np.float64(86.12663338842022), np.float64(86.09021509215812), np.float64(86.05383370034522), np.float64(86.01748914821721), np.float64(85.98118137117838), np.float64(85.94491030480125), np.float64(85.90867588482578), np.float64(85.87247804715906), np.float64(85.83631672787435), np.float64(85.80019186321091), np.float64(85.7641033895731), np.float64(85.72805124352992), np.float64(85.69203536181436), np.float64(85.65605568132301), np.float64(85.62011213911528), np.float64(85.58420467241291), np.float64(85.54833321859942), np.float64(85.51249771521955), np.float64(85.47669809997865), np.float64(85.44093431074212), np.float64(85.40520628553494), np.float64(85.36951396254102), np.float64(85.33385728010266), np.float64(85.29823617672011), np.float64(85.2626505910508), np.float64(85.22710046190903), np.float64(85.1915857282653), np.float64(85.15610632924579), np.float64(85.12066220413179), np.float64(85.08525329235928), np.float64(85.04987953351825), np.float64(85.0145408673523), np.float64(84.97923723375798), np.float64(84.9439685727844), np.float64(84.90873482463253), np.float64(84.87353592965496), np.float64(84.83837182835506), np.float64(84.80324246138666), np.float64(84.76814776955348), np.float64(84.73308769380864), np.float64(84.69806217525417), np.float64(84.66307115514033), np.float64(84.62811457486534), np.float64(84.59319237597485), np.float64(84.55830450016117), np.float64(84.52345088926316), np.float64(84.48863148526536), np.float64(84.45384623029784), np.float64(84.4190950666354), np.float64(84.3843779366973), np.float64(84.34969478304671), np.float64(84.31504554839009), np.float64(84.28043017557695), np.float64(84.24584860759919), np.float64(84.21130078759056), np.float64(84.17678665882652), np.float64(84.14230616472328), np.float64(84.10785924883776), np.float64(84.07344585486689), np.float64(84.03906592664715), np.float64(84.00471940815412), np.float64(83.97040624350214), np.float64(83.93612637694362), np.float64(83.90187975286878), np.float64(83.86766631580507), np.float64(83.83348601041676), np.float64(83.7993387815044), np.float64(83.76522457400459), np.float64(83.73114333298929), np.float64(83.69709500366544), np.float64(83.66307953137446), np.float64(83.6290968615921), np.float64(83.59514693992755), np.float64(83.56122971212329), np.float64(83.52734512405455), np.float64(83.49349312172896), np.float64(83.45967365128597), np.float64(83.4258866589965), np.float64(83.39213209126258), np.float64(83.35840989461674), np.float64(83.32472001572168), np.float64(83.29106240136996), np.float64(83.25743699848327), np.float64(83.22384375411227), np.float64(83.1902826154362), np.float64(83.15675352976217), np.float64(83.123256444525), np.float64(83.08979130728667), np.float64(83.05635806573603), np.float64(83.02295666768822), np.float64(82.98958706108444), np.float64(82.9562491939913), np.float64(82.92294301460073), np.float64(82.88966847122929), np.float64(82.8564255123179), np.float64(82.82321408643143), np.float64(82.79003414225825), np.float64(82.75688562860982), np.float64(82.72376849442047), np.float64(82.69068268874665), np.float64(82.65762816076699), np.float64(82.62460485978143), np.float64(82.59161273521123), np.float64(82.55865173659834), np.float64(82.52572181360509), np.float64(82.49282291601374), np.float64(82.45995499372626), np.float64(82.42711799676374), np.float64(82.39431187526614), np.float64(82.36153657949187), np.float64(82.32879205981737), np.float64(82.29607826673686), np.float64(82.2633951508618), np.float64(82.23074266292058), np.float64(82.19812075375823), np.float64(82.16552937433588), np.float64(82.13296847573056), np.float64(82.10043800913479), np.float64(82.06793792585604), np.float64(82.03546817731669), np.float64(82.00302871505326), np.float64(81.97061949071644), np.float64(81.93824045607052), np.float64(81.90589156299296), np.float64(81.8735727634743), np.float64(81.8412840096175), np.float64(81.80902525363784), np.float64(81.7767964478623), np.float64(81.7445975447295), np.float64(81.71242849678913), np.float64(81.68028925670174), np.float64(81.64817977723818), np.float64(81.6161000112796), np.float64(81.58404991181673), np.float64(81.55202943194979), np.float64(81.52003852488811), np.float64(81.48807714394961), np.float64(81.45614524256074), np.float64(81.42424277425584), np.float64(81.39236969267715), np.float64(81.36052595157415), np.float64(81.32871150480334), np.float64(81.296926306328), np.float64(81.26517031021773), np.float64(81.23344347064815), np.float64(81.20174574190072), np.float64(81.1700770783621), np.float64(81.13843743452414), np.float64(81.10682676498334), np.float64(81.07524502444066), np.float64(81.04369216770107), np.float64(81.01216814967341), np.float64(80.98067292536984), np.float64(80.9492064499057), np.float64(80.9177686784991), np.float64(80.88635956647076), np.float64(80.85497906924334), np.float64(80.82362714234152), np.float64(80.7923037413915), np.float64(80.76100882212071), np.float64(80.72974234035743), np.float64(80.69850425203062), np.float64(80.66729451316947), np.float64(80.63611307990328), np.float64(80.6049599084609), np.float64(80.57383495517072), np.float64(80.54273817646006), np.float64(80.51166952885505), np.float64(80.48062896898041), np.float64(80.44961645355886), np.float64(80.4186319394111), np.float64(80.38767538345544), np.float64(80.35674674270737), np.float64(80.32584597427949), np.float64(80.29497303538096), np.float64(80.26412788331746), np.float64(80.2333104754907), np.float64(80.2025207693982), np.float64(80.17175872263306), np.float64(80.14102429288364), np.float64(80.11031743793316), np.float64(80.07963811565959), np.float64(80.0489862840352), np.float64(80.01836190112641), np.float64(79.98776492509344), np.float64(79.95719531419009), np.float64(79.92665302676329), np.float64(79.896138021253), np.float64(79.86565025619193), np.float64(79.83518969020513), np.float64(79.80475628200979), np.float64(79.77434999041499), np.float64(79.74397077432137), np.float64(79.71361859272092), np.float64(79.68329340469658), np.float64(79.65299516942218), np.float64(79.62272384616197), np.float64(79.59247939427044), np.float64(79.56226177319202), np.float64(79.5320709424609), np.float64(79.50190686170062), np.float64(79.47176949062391), np.float64(79.44165878903242), np.float64(79.41157471681638), np.float64(79.38151723395438), np.float64(79.35148630051326), np.float64(79.32148187664754), np.float64(79.2915039225994), np.float64(79.26155239869831), np.float64(79.23162726536094), np.float64(79.20172848309058), np.float64(79.17185601247725), np.float64(79.14200981419714), np.float64(79.11218984901264), np.float64(79.08239607777178), np.float64(79.05262846140829), np.float64(79.02288696094102), np.float64(78.99317153747404), np.float64(78.96348215219619), np.float64(78.93381876638071), np.float64(78.9041813413853), np.float64(78.87456983865165), np.float64(78.84498421970527), np.float64(78.81542444615526), np.float64(78.78589047969402), np.float64(78.75638228209702), np.float64(78.72689981522254), np.float64(78.69744304101158), np.float64(78.6680119214874), np.float64(78.63860641875529), np.float64(78.60922649500264), np.float64(78.57987211249836), np.float64(78.5505432335927), np.float64(78.52123982071718), np.float64(78.49196183638428), np.float64(78.46270924318709), np.float64(78.43348200379924), np.float64(78.40428008097449), np.float64(78.37510343754677), np.float64(78.34595203642967), np.float64(78.31682584061636), np.float64(78.28772481317934), np.float64(78.25864891727015), np.float64(78.22959811611922), np.float64(78.20057237303568), np.float64(78.17157165140698), np.float64(78.14259591469872), np.float64(78.11364512645467), np.float64(78.08471925029606), np.float64(78.05581824992187), np.float64(78.02694208910827), np.float64(77.99809073170844), np.float64(77.96926414165257), np.float64(77.94046228294735), np.float64(77.91168511967604), np.float64(77.88293261599794), np.float64(77.85420473614838), np.float64(77.82550144443847), np.float64(77.79682270525495), np.float64(77.7681684830598), np.float64(77.73953874239012), np.float64(77.710933447858), np.float64(77.68235256415016), np.float64(77.65379605602789), np.float64(77.62526388832666), np.float64(77.59675602595603), np.float64(77.56827243389947), np.float64(77.53981307721409), np.float64(77.51137792103044), np.float64(77.48296693055231), np.float64(77.45458007105644), np.float64(77.42621730789251), np.float64(77.39787860648283), np.float64(77.369563932322)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "\n",
    "    - generate a regression dataset\n",
    "    - split data into 80% for training and 20% for testing, set random_state to 23\n",
    "    - use Ridge implementation and fit several ridge regression models \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# other relevant imports\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=50, n_informative=10, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "# range of alpha values\n",
    "alphas = np.arange(0.1, 100, 0.1)\n",
    "coefficient_norms = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    model = Pipeline([\n",
    "        \n",
    "        # scale the data and apply ridge\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    # fitting the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # extracting coefficients\n",
    "    coefficients = model.named_steps[\"ridge\"].coef_\n",
    "\n",
    "    # find L2 norm\n",
    "    norm = np.linalg.norm(coefficients)\n",
    "    coefficient_norms.append(norm)\n",
    "\n",
    "print(\"Coefficient Norms: \", coefficient_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f84703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE:  161.9683165445073\n",
      "Best Ridge Alpha:  0.30000000000000004 Ridge MSE:  161.65602954648867\n",
      "Best Lasso Alpha:  0.8 Lasso MSE:  119.60240061543118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "\n",
    "    - performing cross validation on the training dataset to find best model between linear, lasso and ridge\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# linear regression model\n",
    "lr_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LinearRegression())\n",
    "])\n",
    "\n",
    "lr_cv = cross_validate(\n",
    "    lr_model, X_train, y_train,\n",
    "    cv=5, scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "\n",
    "lr_mse = -1 * np.mean(lr_cv[\"test_score\"])\n",
    "\n",
    "\n",
    "# ridge regression model\n",
    "ridge_mse_list = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    ridge_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    cv_result = cross_validate(\n",
    "        ridge_model, X_train, y_train,\n",
    "        cv=5, scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "\n",
    "    ridge_mse_list.append(-np.mean(cv_result[\"test_score\"]))\n",
    "\n",
    "best_ridge_alpha = alphas[np.argmin(ridge_mse_list)]\n",
    "best_ridge_mse = min(ridge_mse_list)\n",
    "\n",
    "\n",
    "# lasso regression model\n",
    "lasso_mse_list = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    lasso_model = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lasso\", Lasso(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    cv_result = cross_validate(\n",
    "        lasso_model, X_train, y_train,\n",
    "        cv=5, scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "\n",
    "    lasso_mse_list.append(-np.mean(cv_result[\"test_score\"]))\n",
    "\n",
    "best_lasso_alpha = alphas[np.argmin(lasso_mse_list)]\n",
    "best_lasso_mse = min(lasso_mse_list)\n",
    "\n",
    "# results\n",
    "print('Linear Regression MSE: ', lr_mse)\n",
    "print('Best Ridge Alpha: ', best_ridge_alpha, 'Ridge MSE: ', best_ridge_mse)\n",
    "print(\"Best Lasso Alpha: \", best_lasso_alpha, 'Lasso MSE: ', best_lasso_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8a2aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Test MSE: 176.49701775114667\n",
      "Best Ridge Alpha: 0.30000000000000004  Ridge Test MSE: 176.601921740432\n",
      "Best Lasso Alpha: 0.8  Lasso Test MSE: 148.67612432257064\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    - evaluate the model on the test set\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# train best ridge on training set and then test\n",
    "best_ridge_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()), \n",
    "    (\"ridge\", Ridge(alpha=best_ridge_alpha))\n",
    "])\n",
    "\n",
    "best_ridge_model.fit(X_train, y_train)\n",
    "ridge_test_mse = mean_squared_error(y_test, best_ridge_model.predict(X_test))\n",
    "\n",
    "# train best lasso on training set and then test\n",
    "best_lasso_model = Pipeline([ \n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lasso\", Lasso(alpha=best_lasso_alpha))\n",
    "])\n",
    "\n",
    "best_lasso_model.fit(X_train, y_train)\n",
    "lasso_test_mse = mean_squared_error(y_test, best_lasso_model.predict(X_test))\n",
    "\n",
    "# train linear regression model on training set and then test \n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_test_mse = mean_squared_error(y_test, lr_model.predict(X_test))\n",
    "\n",
    "print(\"Linear Regression Test MSE:\", lr_test_mse)\n",
    "print(\"Best Ridge Alpha:\", best_ridge_alpha, \" Ridge Test MSE:\", ridge_test_mse)\n",
    "print(\"Best Lasso Alpha:\", best_lasso_alpha, \" Lasso Test MSE:\", lasso_test_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82af108",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'JetBrains Mono', monospace; font-size:16px;\">\n",
    "\n",
    "\n",
    "## Problem 2: Class Weighted Algorithm\n",
    "In this problem, we explore how class imbalance affects model performance by comparing a normal logistic regression model with a class-weighted version on the credit card fraud dataset. We load the data, check how imbalanced it is, scale the features, train both models and then compare their recall on the minority fraud class. \n",
    "\n",
    "### Questions on Class Weighted\n",
    "\n",
    "1. If class 1 is the minority class, how should (v1) be chosen with respect to (v0)?\n",
    "\n",
    "If class 1 is the minority class, we choose v1 bigger than v0 so the model treats class 1 mistakes as more serious. \n",
    "\n",
    "<span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts:  0\n",
      "0    284314\n",
      "1       492\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Proportions:  0\n",
      "0    0.998273\n",
      "1    0.001727\n",
      "Name: proportion, dtype: float64 \n",
      "\n",
      "Recall for Normal Logistic Regression:  0.5795454545454546\n",
      "Recall for Weighted Logisitc Regression: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "    - load the creditcard.csv file\n",
    "    - check propotion of each class and comment on severity of class imbalance\n",
    "    - do the train-test split\n",
    "    - fitting two models on the training data\n",
    "    - test the two models on the testing set\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# importing as a pandas data frame\n",
    "card_df = pd.read_csv(\"data/creditcard.csv\")\n",
    "\n",
    "# selecting the target feature\n",
    "labels = card_df.iloc[:, -1]\n",
    "\n",
    "# how many 0's and 1's and find the proportions\n",
    "print(\"Counts: \" ,labels.value_counts(), \"\\n\")\n",
    "print(\"Proportions: \", labels.value_counts(normalize=True), \"\\n\")\n",
    "\n",
    "# train-test split\n",
    "features = card_df.iloc[:, :-1]\n",
    "label = card_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "    features, label, \n",
    "    test_size = 0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# scaling the data \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# logistic regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# weighted logisitc regression\n",
    "log_reg_weighted = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "log_reg_weighted.fit(X_train_scaled, y_train)\n",
    "\n",
    "# performing predictions\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "y_pred_log_weighted = log_reg_weighted.predict(X_test_scaled)\n",
    "\n",
    "# recall scores\n",
    "recall_normal = recall_score(y_test, y_pred_log)\n",
    "recall_weighted = recall_score(y_test, y_pred_log_weighted)\n",
    "\n",
    "print(\"Recall for Normal Logistic Regression: \", recall_normal)\n",
    "print(\"Recall for Weighted Logisitc Regression:\", recall_weighted)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs3400_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
